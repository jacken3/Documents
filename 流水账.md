今日目标：

1、修改损失函数，训练跑上

      * 感觉loss_mem_bank_DDQ基本功能实现，待会过一下流程理一下思路。
      * 现在需要把一对多的损失加上
    * 首先找到一个Assigner能够实现一对多的匹配 找到一个 maxIOU Assigner
        对应需要修改get_target()函数，调用新的Assigner去做匹配
        注意一下bbox的格式，在做匹配和损失计算的时候用的格式不一样
    * 损失的计算应该是一致的，直接调用loss_single_decoder(),这里待会可以再看看实际的运行流程。

存在的问题：
	一对多的匹配效果不好，没有想象中的几百个框匹配上GT的现象发生。数量甚至不如一对一的匹配。	
	其次，be_distinct杀框效果也一般。
另一种可能的匹配方案：
	把GT框投影到2

2、把之前的Stream PETR的bug修改一下，再跑一版本的实验。
  今天必做 完成

3、CAPE源码看一下，看一下能不能加进来

2023.08.01

今日任务：

1、论文阅读：DEYO2

2、把DDQ的代码先跑一版本起来，检查一下代码问题。思考一下其他的匹配方案。
	IOU卡一个大于零就行？
	还有一个问题，Track_query在做一对多匹配时应该采取什么策略，是直接和GT计算损失还是同样参与一对多的匹配。
	
	目前考虑的实验方向：
	   1、最重要的，一对多的匹配方式，如何保证GT框是会参与多个预测结果监督的？考虑改IOU的阈值进行多组实验
	   2、正负样本的可视化，可以看一下匹配结果是否符合预期，这部分可以做出来用来验证实际的匹配效果
	   3、distinct卡的阈值的相关实验，设置不同的阈值会影响被保留Query的数量，但考虑到会选topk,认为这个对整体性能影响不会很大。


3、基于距离的绑定，基于均匀分布ref_point的绑定。 



2023.08.02

今日任务：
1、学习增长：DEYOv2、nuScense数据集处理代码

2、把绑定关系可视化出来，测试一下当前性能表现。
	* 先说性能测试，需要先把测试流程过一下，这部分基本完成
	* 绑定可视化需要实现：
		1、标出gt的ID
		2、标出被绑定为正样本的预测狂对应的gt的id
		3、颜色区分
3、优化代码，Debug，考虑进行下一步多帧训练的相关实验。
	*整理一下transformer的结构，统一一下  完成🐕
	*整理一下之前的几个部分的代码和DDQ的实现，入口控制变量写的清楚一些。

4、尝试按照固定栅格初始化参考点：具体逻辑如下：
	将pc_range在BEV空间上划分为一定数量的栅格
	每个栅格对应初始化一个参考点以及Query
	当GT框与栅格存在重合区域时就将两者匹配在一起去计算损失。
     关于这部分的思考：首先这么做就意味着一个栅格只能存在一个query，只会预测一个物体，考虑一种情况，当多个gt同时出现在一个栅格里面时，
匹配怎么实现？而且这个时候的检测该怎么做？

思考：

单帧训练对于post reasoner来说意义不大，因为没有和历史有效的query做交互，出来的结果很差可以理解，但是本质上来说，模型应该是
能够训练一个比较好的一对多的检测头的，可以可视化一下看看效果。


为了去除大部分的背景冗余框：
topK应该选非背景，不考虑背景作为一个类别。


Let’s think step by step
简单----》复杂 能不能先预测前景和背景，做筛选



一对一匹配的缺点：


问：Dense的Query为什么不能做一对一的匹配？

A:Dense Query中间存在相似度很高的query，而由于一对一匹配，这些相似度极高的Query往往会被分成不同的前景或者背景。
要从相似度极高的query上做区分是不合理的，因为他们出的结果本来就应该是一致的，这对模型优化是不利的。



2023.08.03：
今日任务：
1、把之前系列的实验重新跑一下，过一下流程
	具体实验安排：

单帧：base、fpe、fpe+hist、fpe+stream、stream、stream_256

| 实验名称         | 实验路径                                                     | 模型训练状况 | 点数测试情况                          |
| ---------------- | ------------------------------------------------------------ | ------------ | ------------------------------------- |
| base_500单帧     | ckpts/ckpt_detec/base                                        | 完成         | 完成 0.3814                           |
| base多帧         | ckpts/ckpt_track/base                                        | 完成         | 完成  0.406                           |
| fpe_500单帧      | ckpts/ckpt_detec/fpe                                         | 完成         | 完成 0.3936                           |
| fpe_500多帧      | ckpts/ckpt_track/fpe_500                                     | 完成         | 检测点数：0.4773<br />跟踪点数：在测  |
| fpe+hist多帧     | （预训练模型拿fpe）看看500query能不能跑通，不行跑300（正在跑300 query） | 完成         | 检测点数：0.4657<br />跟踪点数：0.397 |
|                  |                                                              |              |                                       |
| stream多帧       |                                                              |              |                                       |
| stream_256多帧   | （预训练模型拿fpe）看看500query能不能跑通，不行跑300 ckpts/ckpt_track/stream_len_256 | 完成         | 检测点数：0.4721<br />跟踪点数：0.399 |
| base单帧24 epoch | ckpts/ckpt_detect/base                                       | 完成         | 用于和DDQ_one2one做检测点数的对比     |



	多训练一个单帧检测模型，基于BASE的检测结果在做训练。

2、写一个基于栅格的一对多匹配方案，试一下效果

	copy多个GT，保证相似性很高的框也能做同样的匹配。


​	
​	或者强制按距离绑定，保证每个GT都有预测框匹配。(完成)

3、把DEYO系列读完，看看具体实现





2023.08.04

1、基于copy GT的一对多匹配实现
	这部分实验目前正在跑，之后可以看看点数，预训练模型是base_12单帧检测模型。
	

2、HDETR的相关思想

3、看一下正常模型500个query出框和1000个框筛选30个框的区别
	这部分还包含后续实验如何进行，待会需要考虑一下
	先跑一下多帧跟踪的debug,然后考虑训练

​	:star2:之前这里的训练方式是先做了distinct的操作，然后再去做的一对一匹配

post reasoner那块靠的是之前一对一的匹配结果，也就是说，多了一个post reasoner以及多了Query，看看多帧效果先

![image-20230804145940613](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20230804145940613.png)



2023.08.07

今日任务：

1、整理一下实验结果，分析一下原因，帖子更新一下
	包括数据和可视化
be_distinct需要修正,能不能保证被匹配上的预测框至少有一个被保留到下一阶段，同时分数不应该考虑背景
增加query数量

2、post reasoner引入图像信息交互
	代码修改完成，风险：可能会对之前的代码逻辑造成从冲击

3、HDETR的代码出来

把现在DDQ的代码实现保存一版本，然后重开一个分支来做HDETR的事情，记得先把代码检查好。

​		首先第一种实现：类似于DN分组，aux分支的监督改为一对多，这部分的实现考虑利用copyGT来实现。只有正常分支的结果会送到track_instance给下游post reasoner进行监督

![image-20230807171651964](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20230807171651964.png)

当前的DDQ实现和hybrid_branch应当是不兼容的。

​	其次第二种实现：类似于当前的结构，需要考虑track_instance的更新是基于Dense的还是sparse的。

![image-20230807172009660](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20230807172009660.png)

2023.08.10
1、数据流重处理，单帧流式逻辑，帧间detach逻辑。

先处理数据，在做代码移植，搞清楚训练集数据加载的逻辑。



2、回顾一下实验，复盘和考虑下一步的方向

- 多帧DDQ_copyGT方案检测涨点明显多帧，跟踪正在测试

- 目前还在训练的模型有单帧：base_1000,hybrid_layer_wo_past。
- 可以再训一个hybrid_branch_1000_500的模型。   

3、今天把论文读完(Done)





2023.08.14

**今日待做**

1、把多帧的实验看看能不能再跑一个

尼玛，模型丢了，



2、hybrid_4加筛选代码写完，单帧实验跑起来



​	这块完成



![image-20230814163001874](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20230814163001874.png)



3、CAPE代码阅读，准备着手开始改动。



2023.08.15

**今日待做**

1、hybrid方案的多帧模型实验可以做一下

待训练多帧模型：hybrid_layer_1000_topk300



2、CAPE代码修改要完成



3、总结实验，参加周会



2023.08.16

branch的跟踪数据放错地方

![image-20230816102654138](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20230816102654138.png)

待做：

1、完成CAPE代码修改

​	Done

2、修改DN带来的bug



3、流式训练方式考虑。

发现一个问题，之前topk300的训练方式是顺序训练的，存在问题。

![image-20230816192820213](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20230816192820213.png)





2023.08.17



待做

1、修改DN带来的BUG

好像没有问题，不用修正。

2、把加上shuffle之后的修正实验跑了

多帧模型正在训练

3、CAPE单帧模型点数测试

待训练

4、按照stream viedo的方式训练一个多帧模型

习武师兄在做，暂时搁置

5、阅读3DPPE的代码

考虑三个问题：

- 深度的监督信息怎么引入的

明天看看这个信息能不能直接引入

![image-20230817193117212](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20230817193117212.png)

- 深度预测的结果是如何作用到PE上的。

![image-20230817193251720](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20230817193251720.png)

- 深度监督的引入

两个地方，

2023.08.18

完成3DPPE代码的阅读和修改工作

- 数据格式处理完毕
- 

长期任务：阅读pytorch的源码。





2023.08.21

- 完成3DPPE代码的整理

​     完成

- CAPE出个结论
  目前有一个问题，就是之前提到的base多帧的模型都是在，考虑训练一个用24个epoch单帧模型作为预训练模型的多帧模型。

​	    正在训练

- 开题研究背景和国内外发展现状写完

2023.08.22

- 考虑一下把CAPE和Hybrid_layer联合起来跑一个实验，先从单帧模型开始训练起。

​		CAPE时序feature的对齐实现。

- 3DPPE检查一下点数崩掉的原因

- 



2023.08.23

- 另外可以补一个对hist_embed做对齐的实验

​		在尝试

- 3DPPE GT出检测结果

​		这块代码上可能有点工作量

- CAPE多帧调整

​		这块的实验正在跑，检测点数还可以



国内外研究现状：

2023.08.24

- 把CAPE的评测在过一遍，把检测点数测一下，确定一下跟踪点数的正确性

​		这块检测的点数还在测，待会看一下是不是0.49，过一下数据的正确性

​		确定检测点数是没有问题的，从侧面印证跟踪点数应该也没啥问题。

- 3DPPE的代码过一遍，把整个代码逻辑在理一遍，询问点堃关于深度的使用

​		今天的主要任务是这个，检差一下代码问题，过一下训练逻辑。



​		写一个关于深度图可视化的程序，参考B站。



![image-20230824184121763](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20230824184121763.png)

- 自车的运动对齐特征，这个实验可以测一个点数

​		这边需要注意的是，把hist_embeds和query都对齐了，现在的代码把hist_embed对齐的部分给注释掉了。

​		这部分实验结果出来了，检测轻微掉点，跟踪点数还在测试。

2023.08.24

- 明天详细对一下3DPPE的代码，感觉还是有问题。

​		出一个Lidar-Ray的实验结果，排查一下问题出在哪儿

- CAPE+Hybrid_layer跑一个多帧的实验。
  - 这块显存炸了，感觉CAPE的Attention机制对显存不是很友好

- 确定的一个结论：query_feature对齐可以去掉不用，感觉作用不是很大

- 然后今天可以进一步对一下3DPPE的代码结构和逻辑，主要是数据预处理的过程。

![image-20230825152242740](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20230825152242740.png)

从这段代码来看：Lidar2Img得到的实际是从Lidar到增广后的图像UV坐标系的变换矩阵。

![image-20230825152356423](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20230825152356423.png)

同时，和3DPPE不太一样的是，在得到lidar2img的转换矩阵时，PF-Track的实现里面对外参多做了一个转置？不是很理解，可以看看他的外参矩阵是怎么得到的。

检查发现是一个代码遗留问题，CAPE里面也有提到，通过dataset的处理最终得到的实际是外参矩阵的转置，所以这个外参矩阵不能直接使用，但是在3DPPE中，在dataset环节就已经做了转置了。



相机内参：uv--->相机坐标系

相机外参：相机坐标系--->Lidar坐标系（在PF-Track中起码是这样的）

:star::同时，需要注意的是，拿到的相机外参实际是没有做过转置的，我们要用的话需要做一步转置。



同时，我们拿到的最终的lidar2cam的矩阵变换是从点云坐标到增广后的cam坐标的变换矩阵，感觉3DPPE的原始处理存在问题。

Ans



解析一下我们拿到的数据：

- extrinsics：实际是从Lidar到cam的变换矩阵的转置
- intrinsic：是相机内参cam-->uv，没有问题
- lidar2img_rts:是lidar到uv的变换矩阵

![image-20230825155539999](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20230825155539999.png)

![image-20230828105415615](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20230828105415615.png)

![image-20230828105505584](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20230828105505584.png)

![image-20230828154314867](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20230828154314867.png)

2023.08.29：

1、还可以改进的地方：参考点PE和2D特征PE共享参数



2023.09.07：

1、Debug 3DPPE和迁移代码的实现，详细比较一下问题出在哪儿

​	1、在3DPPE里面，好像图像没有归一化

​     2、Crop的方式不一样

​	 3、backbone精度

2、找一篇论文阅读，准备明天的Paper Reading



2023.09.11：

1、阅读SparseBEV论文

​	Q:如果结合Deformable Attention的思想，把PETR的Decoder也改成可变形的呢？



几个涨点的策略：

​	1、把3D的参考点改成BEV Pillars

​	2、引入自适应多尺度自注意力机制,这块对检性能提升很大，本质上是用到了多尺度特征的能力。

​	3、可以把Decoder的交叉注意力也设计成Deformable的从而进一步减小计算量，后期在多模态中也可以尝试这个方案。

2、看一下PF-Track的可视化代码，做几个可视化的结果，分析一下问题，要思考而非follow.

​	这部分出的有点慢，而且不适合定性分析结果，可以参考他的原始代码，自己出一个。

3、阅读CMT的代码，考虑一下组会

4、multiPath++，嘉恒日报



2023.09.12

1、完成SparseBEV的初步迁移

​	把所有的tensor的Size标一下，做一个统一

2、总结一下3DPPE做的测试行工作，准备下午的周会

3、multiPath++，嘉恒日报

4、有时间的话把整个代码框架整理一下，统一一个比较稳定的输入输出接口。

5、针对3DPPE的代码，测试一下把深度编码去掉，只引入深度监督的部分。



2023.09.13

1、针对3DPPE的代码，测试一下把深度编码去掉，只引入深度监督的部分。这部分训练先跑起来。

- 只引入深度的辅助监督，对img Backbone进行学习，预估的实验结果是不影响模型的基本性能。

实验正在进行，后续看结果

2、SparseBEV的实验跑起来



- 训练正在进行中。

- 做一个消融实验

  | Adaptive self attention | sampling_method |
  | ----------------------- | --------------- |
  | ——                      | ——              |
  | √                       | ——              |
  | ——                      | √               |
  | √                       | √               |

2023.09.14

1、深度实验如果只引入深度监督，但不影响编码方式的话，涨点结论不明显，在该e12模型的基础上，尝试训练一个e24的模型。

| method     | pre    | epoch | NDS    | AP     | ATE    | ASE    | AOE    | AVE    | AAE    |
| ---------- | ------ | ----- | ------ | ------ | ------ | ------ | ------ | ------ | ------ |
| base       | vovnet | 12    | 0.3814 | 0.3532 | 0.793  | 0.2725 | 0.6027 | 1.1112 | 0.2843 |
| +depth监督 | vovnet | 12    | 0.3903 | 0.3394 | 0.7965 | 0.2755 | 0.4712 | 0.9870 | 0.2634 |

2、SPARSE BEV的消融实验。

| Adaptive self attention | sampling_method |
| ----------------------- | --------------- |
| ——                      | ——              |
| √                       | ——              |
| ——                      | √               |
| √                       | √               |

1、4完成

2：flag:ada_sAtten

3、prediction部分的代码阅读

2023.09.15

1、3DPPE后续实验的推进

​	1、继续把仅引入深度监督，但不做深度PE的模型训练到24个Epoch测试性能。（周末）

​	2、在上面的基础上训练一个多帧的模型。（还是不加深度PE）（周末）

​	3、在1的基础上引入深度PE训练一个多帧的模型。(多帧加深度PE)(周末)

​	4、在12-12DPE模型的基础上训练一个多帧模型。（周末）

2、SparseBEV的后续实验推进

​	1、检查显存占用问题

​	2、增加采样点数量，观察点数变化。

2、ada_self Attention的可视化

​	tau值分布不成立，没有学到多尺度

3、Paper Reading



2023.09.18

1、阅读论文HIVT和QCNet



2、整理SparseBEV的实验结果和3DPPE的实验结果





2023.09.19

1、阅读UniAD代码和motionFormer模块



2、decoder拆分，前几层只输入DetQuery ,后面几层加入TrackQuery。

​	解决

3、SparseBEV增加图像特征的尺度

![image-20230919191437857](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20230919191437857.png)

​	同时在base_line的基础上开展实验 太难改了，放弃，有时间再改

2023.09.20：



1、排查一下训练速度的问题

​	上午解决：
​		1、确定是否代码问题 

​			可以用之前的配置文件跑PF-Track-git的代码，这边应该是没有问题的，深度网络没加进来过。

​		2、申3090跑实验



2、发现训练SparseBEV的实验时，出现参数名称对不齐的情况，不知道是不是保存的时候出问题了。

![image-20230920110845238](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20230920110845238.png)

定位到bug:

![image-20230920115032987](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20230920115032987.png)

![image-20230920115023012](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20230920115023012.png)



2、预测代码今天要开始上手了







2023.09.26：



1、m2m的顺序可能需要修改一下



2、多组PE的组合方式：参考MTR

​	这个地方实验正在进行，注意，目前进行的实验是轨迹监督的。



2023.09.27：



1、检测尝试可视化一下



2、CMT代码稍微跑起来



3、预测这边的主要任务：

​	Note:目前后续所有进行的实验均是基于轨迹监督的，我们首先要对齐原先的监督方式和现在轨迹监督方式的优劣，为此，需要拿到轨迹监督base_line的预测性能指标。

​	目前在跑的实验有：

- 速度信号监督：
  - base+acc，与最初的baseline完全对齐，区别在于原始的baseline模型motionFormer的decoder内部的回归分支原本出的是速度信号，为了和decoder外部统一，将其改成出加速度信号，也就是说内部做了两次积分。(Done,点数已经记录)

- 轨迹信号监督==（负对数似然损失没有加上去,可以通过下面的开关打开这个部分）==
  
  ![image-20230928104121480](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20230928104121480.png)
  
  - baseline模型，认为内部出的是速度信号，外部出的是加速度信号，相较于最原始的baseline模型只改了监督方式。（09.28 10:37 还在训练）
  - feats_all模型：把track_query换成了detec_query。（Done）
  - base+speed：认为回归分支出的是速度，内部积分一次，外部不积分。（Done）
  - MTR: 把MTR的思想给迁移过来了，用的是轨迹监督。（Done）
  - TC：引入TC监督。（正在训练）
  - ade_0.25，原始的minade的损失权重为0,这里给改成0.25了
  - 把负对数似然损失加上的轨迹信号监督baseline模型：应该拿这个和原始的baseline模型对齐。(正在训练)

目前来说，两种监督方式的实验是没有办法开关控制的，现在只能拿轨迹来做监督。

:star2:：一个比较重要的实验结果是base-traj和base的预测性能比较，在确定base_traj的性能强于base后，基线可以对齐到base_traj。（这个实验需要优先出一个结果）

除此之外：需要进一步考虑的任务有：

- m2m和a2a的顺序对模型性能的影响
- img_feats的实验能不能跑一个。

- 单模态的输出结果如何用到多模态预测中（作为一组Anchor来初始化多模态的）
- TC监督的引入 （这个部分的代码改完了）





2023.09.28:



1、今天实验先不补充了，直接先把一部分代码给改了，考虑如下代码的实现：

- 如何将单模态的检测结果用到多模态检测中
  - 作为一组Anchor加到多模态预测中（显式作用）
  - 依据prediction模块输出的未来轨迹作为PE去修改Track query，生成时序的track feats用于不同时刻的轨迹预测。（隐式作用）
    - MTR内部注意力机制待改，其他工作已经完成

2、CMT的环境准备一下，跑一个base_line的模型加可视化

3、论文阅读：NERF及其代码





2023.10.07:



1、把国庆之前跑的模型结果总结一下，更新到帖子中去。（Done）

2、依据prediction模块输出的未来轨迹作为PE去修改Track query，生成时序的track feats用于不同时刻的轨迹预测。（隐式作用）（代码部分上周应该改的差不多了，今天改完，实验跑出来）(Done)

- MTR内部注意力机制待改，其他工作已经完成

​	正在debug,下午3点之前争取把实验跑起来

3、CMT代码跑起来，需要一个baseline模型，周二前完成。



2023.10.08：



已有代码待进行的实验计划：

m2m&a2a的补充实验：只要m2m，只要a2a，交换m2m和a2a的顺序

预测待做内容：

- 单模态+多模态 时序query在T维度上的交互。

- 多组PE组合调优
- 终点之间的交互
- 单模态预测结果作为多模态预测的Anchor
- 上一时刻的预测输出作为下一时刻的Anchor
- 图像特征的引入（代码已经有了，但是跑不起来）

2023.10.09:



1、把ref_traj给改进来，做ref_traj的偏移。这部分代码需要适配future。

​	在ref_traj的基础上做偏移，reg分支出的是相对于每个时刻ref的偏移量，这里不再有速度的概念。内外需要统一。

2、CMT代码今天必须跑起来，得到一个baseline。或者把nuScenes数据集的处理方式整理一下。

3、找点预测的文章阅读。



2023.10.10:

1、把单模态预测的结果作为一个Anchor加入到多模态预测中来

2、Future增加模态和时序上的交互。（显存代价太高，放弃）



先在baseline上验证一下那种推理方式的点数更高，后期统一为一种推理方式，这部分没有开关变量，只能通过直接改代码实现，所以测试的时候需要注意这个部分。

![image-20231010142007935](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20231010142007935.png)

考虑实现一下新的future的方式，只增加query的数量，而不增加时间维度。（Done）

3090-6: ref_traj

debug_35 :ade_0.5_nll_0





2023.10.11：



![image-20231011140930543](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20231011140930543.png)

发现一个代码BUG，这里应该是想把上一帧匹配上且GT出现在当前帧的GT的ID索引继承到当前帧，但是，索引用的是上一帧的索引。

1、TC的代码查一下。

​	需要改动的地方比较多，比如两次检测结果的模态怎么对齐，相邻两帧坐标系的统一

​	历史的轨迹分为两个Decoder层出的共12个时刻的轨迹

2、单模态预测结果作为一组锚框加到原始的6个模态里面。



2023.10.12：

1、track_query和detec_query解耦，通过相似度匹配来更新track query。





2023.10.13：

1、迁移DQ Track 到PF-Track

- 首先完成检测
- 检测结果与track_let匹配

本质是通过基于学习的方法将之前先检测后匹配的方法搬过来了。

DQ_Track存在的问题：

​	1、track query存在的意义只有与detec query做匹配，在训练阶段，完全没有对检测结果做出修正或其他融合方式。

​	2、学习阶段track query没有消融机制。但是在推理阶段，track query存在消融机制，当track多次没有匹配上检测的detec query时，就会被重新初始化。两者好像没有对齐。



我设想的训练和推理方式：

- 训练阶段：

  1、detector首先对输入的图像进行处理，得到初步的检测结果以及对应的query features。

  2、将当前检测的pos的detec_query与valid的track_query做匹配。（如果track_query没有valid的则直接将当前的检测query作为track_query的部分初始化，这种情况一般出现在首帧）。

  3、根据匹配结果更新track_query的特征，并利用新的更新的特征来修正检测结果。同时，结合了历史信息的track_query能够更好的解决遮挡问题。track_query从某种意义上来说融合了历史时序的信息，与PF-Track相比，PF-Track结合历史时序信息的方式则是通过与hist_embed交互实现的。

- 推理阶段

  1、detector首先对输入的图像进行处理，得到初步的检测结果以及对应的query features。依据分数卡阈值选择正样本。

  2、将当前检测的pos的detec_query与valid的track_query做匹配。匹配结果也需要确定一个阈值来最终得到。

  3、后续与训练阶段保持一致即可。



2023.10.16:

1、迁移DQ-Track到PF-Track。

2、TC方面更细致的实验，包括TC引入的时机等等。 计划采样的控制。评测代码看一遍，出一个时序一致性的指标。

3、基于参考点图像采样的实现。



2023.10.17:

1、时序一致性计划采样，前epoch GT训练

2、future_anchor 只出六个模态，不增加额外的模态。

3、Perceptron代码梳理





4、



2023.10.18



![image-20231018142746359](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20231018142746359.png)

![image-20231018142804302](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20231018142804302.png)

perceptron上的一些问题：

1、detection head基于CMT的实现，内部已经包含了DN的实现，但是按照现在的代码逻辑DN只能在单帧检测的时候使用



![image-20231018173521928](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20231018173521928.png)







总体代码逻辑：

1、检测的head里面实现DN的逻辑，本身有DN损失的计算，外面只需要补充一个额外的损失计算函数即可，主要调整det head即可。（完成）



1、时序一致性计划采样，前epoch GT训练(完成)

2、TC多模态，匈牙利匹配计算损失。 （完成）

3、future_anchor 只出六个模态，不增加额外的模态。

计划采样的控制。评测代码看一遍，出一个时序一致性的指标。



2、perceptron代码迁移：

- DN Done
- copyGT Done
- RFS Done
- aug

修改numpy至 1.20.0

修改mmdet至2.24.1





2023.10.20：

几个比较简单的改进的点：

- query_feats替换为future_embeds（future_fuse）



2023.10.23

**To Do List**

1、CMT BaseLine的代码需要跑起来。（pkl文件还在生成中）

2、Perceptron代码迁移需要结束。（DN有问题，待检查，可能是硬件问题）（把这个弄了）(检查没有问题，估计是训练策略的问题）（完成）

3、PF-Track prediction优化，基于特征点采样的实现。（今天的主要任务）

4、query_feats替换为future_embeds（future_fuse）（最简单的实现，优先进行）（这个完成，正在训练）

4、NERF阅读及笔记（完成）

**Problems**

针对2：DN的代码运行失败，第一个epoch运行结束后直接卡死，比较奇怪。

（可以Debug检查一下）



目前tc_matching以及计划tc的评测正在走，看结果tc可以补计划轮数的实验



2023.10.24:

1、阅读prediction_eval部分，增加TC的指标。

2、增加区域采样。

3、继续思考单模态预测和多模态预测融合。



2023.10.25:

Do One by One:

1、等待copyGT 1200的代码测试完成，如果没有问题则rebase代码，否则Debug代码问题。（Done）

2、Debug prediction_eval的代码，加入tc指标，进行测试。

​	问题在于怎么拿到前后两帧的预测结果，比较困难。

3、查询代做清单，一步步完成工作。



2023.10.26：

1、把可微滤波器加上，跑实验

2、针对图像的可变形注意力机制采样。





2023.11.1:

1、可视化分析TC、REF、BASE

2、可微滤波器排查BUG（Done）

3、Perceptron代码检查

4、图像位置采样







GT的未来预测轨迹是基于Lidar坐标系的。

![image-20231108105722675](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20231108105722675.png)

同时，单模态预测的监督信号是Lidar坐标系下的速度：

![image-20231108110001874](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20231108110001874.png)



而多模态预测的监督信号是以Agent为中心但是方向与Lidar对齐的轨迹。

![image-20231108110136732](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20231108110136732.png)

（可以看到，减掉了一个起始位置，使得轨迹从零开始）





1、出未来的框

2、





2023.11.13

1、把DF的参数分析一下，然后跑一个freeze motionFormer的实验。

![image-20231113140245856](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20231113140245856.png)

模型学到的r_conv的量级很大，这导致初始化的参数基本不起作用，模型只在前后两帧预测置信度间进行参考。

准备跑一个Freeze motionFormer的实验

2、把以object ref_point作为起始点的方案再跑一下

​    在运行中		

3、增加新的指标，便于分析问题。

   左转右转的预判性说明

4、增加车的加速减速，左转右转预测指标

把轨迹预测问题变成一个未来框的目标检测问题



![image-20231114151427635](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20231114151427635.png)

![image-20231114151748125](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20231114151748125.png)







预测的结果同步到检测的参考点的更新里面来，可以基于匀速模型来做。





在单帧检测器模型上，仅参考点一致，学出来的特征就基本保持一致。说明模型学到的特征基本高度和参考点是相关的，给定了参考点，模型出的特征也就基本给定。

结论1：PETR HEAD最后的特征高度与特征点相关，历史信息占的比重本身就不大，高度与当前帧的参考点选择有关，这使得即使特征不是0初始化，模型最后学到的特征也基本保持一致。





intent意图确定：

1、模型出的轨迹是Lidar坐标系下的结果。因此意图信息希望也在Lidar坐标系中表达。

2、意图预测任务具体：

​	将其转化为一个多模态的分类认任务，类别包括：

​		静止、直行、左转、右转，对向直行

![image-20231115175607224](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20231115175607224.png)

将障碍物的运动分为3类，静止、线性运动（匀速）、非线性运动（加减速）



11.16：

解决一个RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn 的报错问题。

当Freeze住motionFormer时，整个网络中可学习的部分只有df,但是DF模块在一次前向过程中可能不会被调用，这导致最后所有的loss的require_grad都是False，从而导致这个报错。



遇到

NCLL的问题

![image-20231116172609308](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20231116172609308.png)



可以系统学习一下NCLL以及分布式的相关内容。





1、解决NCLL的问题

2、修改检测推理逻辑、考虑自适应特征融合

3、query的embed是



1、创建一个solid分支，仅加入确定有效的trick，保证代码干净

2、在dev分支测试各个Anchor的重要性







参考点很重要，一定要准，考虑将预测结果对齐到检测跟踪的参考点更新。



1、测试各种Anchor的重要性，同时检查一下图像特征采样的bug

2、跑一个DF不同初始化的实验

3、增加intent任务的指标。





2023.11.27：



1、各种level的Anchor多做几组实验

只用scenes_level的点数好像不是很高，有点在意料之外

2、增加intent相关的指标，同时修改一下intent预测的损失函数，改成Focal Loss

3、更新一下实验帖子，整理一下前期的实验结果和代码



后续的主要任务：

将PF-Track的逻辑迁移到streamPETR里面去，保持一个较高的检测点数的同时引入跟踪预测任务



相比于PF-Track









2023.11.30:

streamPETR迁移：

1、首先完成PKL文件的统一，在PF-Track中，增加了forecasting信息，同样还有radar的信息。我们也可以把radar的信息迁过来用。

streamPETR：

![image-20231130115542076](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20231130115542076.png)

PF-Track:

![image-20231130122210401](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20231130122210401.png)

streamPETR相对缺少的key包括：forecasting_locs、forecasting_masks、forecasting_tokens、forecasting_types、instance_inds





（1）先同步pkl文件里面的info信息 （Done，在字典info里面新增forecasting_locs、forecasting_masks、forecasting_tokens、forecasting_types、instance_inds信息）

（2）同步get_anno_info把跟踪要用的ID信息以及未来预测信息拿过来

（3）同步数据pipiline流程，保证最后送到网络里面的数据是一样的。

streamPETR：
![image-20231130150525723](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20231130150525723.png)

PF-Track:

![image-20231130151146590](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20231130151146590.png)

- LoadAnnotation3d需要修改
- RangeFilter也需要修改
- NameFilter也需要修改
- CropFlip不需要修改
- Normalize不需要修改
- Pad不需要修改
- collect3D需要在config文件中修改

2、代码层面统一

1、保存trackinstance供跟踪使用

2、损失计算的修改

3、训练方式变为流式训练，track query清空的机制需要与data['prev_exist']对应



好像没有显式的更新缓存机制的代码。







2320.12.04:



1、StreamPETR训练部分代码完成迁移

2、增加intent信息统计

3、对长时序的内容进行测试



![image-20231204111828495](C:\Users\zhurunwen\AppData\Roaming\Typora\typora-user-images\image-20231204111828495.png)

这是新拿到的数据


2023.12.06:

TODO:

1、完成代码的修改工作，训练顺利跑起来，目前主要存在的问题有下面几个：
  mem的loss不太对
  没有办法多batch训练
  没有考虑instance的结构问题



2023.12.07

1、在修改好的代码上训练一个原始版本的实验，对齐一下点数，确定代码没有问题。
2、继续修改一下没有目标时训练出现的bug

待会看一下没有修改的streampetr是如何处理多batch问题的，我们后面可以改过来

self.tracking参数：控制是否执行tracking的任务，如果不执行tracking的话，按道理来说，实验结果应该和原始的streamPETR的点数保持一致

执行prediction loss计算的时候，出现了有部分参数没有得到回传梯度的问题，具体原因是样本中出现了没有bbox的情况，导致loss计算取active instaances的时候没有值，导致最后计算损失没有与模型相关。
这个比较麻烦,感觉PF-Track里面在数据处理的时候应该增加了过滤没有bbox的代码。否则代码绝对有问题。


多batch训练暂时搁置一下，因为PF-Track的方式多Batch比较困难，考虑重新训练一个